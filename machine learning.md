# 统计学习方法概论

## 监督学习

## 统计学习三要素

统计模型的三要素：模型+策略+算法

## 模型评估和模型选择

### 模型评估与模型选择

### 过拟合和模型选择

过拟合是指学习时选择的模型所包含的参1数过多，以至于出现这一模型对于已知数据预测的好，但是对于未知数据预测的差的现象。解决过拟合的方案是正则化和交叉验证。

## 正则化与交叉验证

### 正则化

- L1范数：得到稀疏的权值？。e.g. LASSO
- L2范数：得到平滑的权值？。e.g. Ridge Regression

### 交叉验证

就是将数据分为训练集和测试集

## 泛化能力

Error = bias + variance + noise

模型搞得复杂，参数多，才能低偏差，模型简单，参数少，才能低方差，[偏差和方差](https://www.zhihu.com/question/20448464), 简单讲偏差是距离目标值的接近程度，方差是预测值的变化范围，预测值与预测值之间的变化程度

## 生成模型与判别模型

### 生成模型

- 生成模型 ： 数据-> P(X,Y) ->P(Y|X)
- 常见的生成模型：朴素贝叶斯和HMM，混合高斯模型

### 判别模型

- 数据 -> P(Y|X)
- 常见的判别模型：KNN，感知机,CRF,逻辑回归，SVM，最大熵模型

## 分类问题

评价分类器性能的指标一般是分类准确率（accuracy），就是正确分类的样本数占总样本数之比。
对于二分类问题：常用的是精确率（precision）和召回率（recall），所分类的情况有四类：将正类判定为正类（TP， true  positive），N（FN，False negative），将负类判定为负类（TN），将负类判定为正类。（FN）。

精确率定义:就是占所有预测结果中正分类数的比例

$$P=\frac{TP}{TP+FP}$$

召回率定义:就是占本来是正分类数的比例

$$R=\frac{TP}{TP+FN}$$

F1值：是准确率与召回率的调和均值

AUC与ROC的：解决样本不均衡的问题，AUC与ROC的内容

## 标注问题

标注问题常用的统计方法是：HMM，CRF

## 回归问题
# 感知机

# KNN

# 朴素贝叶斯

决策的结果是可能性最大的那个

# 决策树

## What

## Why

(例子来源：李航统计学习方法）

| ID   | 年龄   | 有工作  | 有自己的房子 | 信贷情况 | 类别   |
| ---- | ---- | ---- | ------ | ---- | ---- |
| 1    | 青年   | 否    | 否      | 一般   | 否    |
| 2    | 青年   | 否    | 否      | 好    | 否    |
| 3    | 中年   | 否    | 否      | 一般   | 否    |

决策树+集成 = 随机森林

## How

# 逻辑回归与最大熵模型

logistic 回归模型：

1. $$ z= W^{T}+b$$
2. $$ \hat{y} = \sigma(z)$$
3. sigmod函数 $$ \sigma(x) = \frac{1}{1+\exp(-x)}$$



# 支持向量机(SVM)

### SVM 核函数

- 多项式核函数
- 高斯核函数
- 字符串核函数
- 径向基核函数（RBF）
- sigmoid核函数
- 拉普拉斯核函数

# 提升方法

## 提升方法与集成方法

ensemble就是把几种机器学习的算法组合在一起或者是把一种算法的不同参数组合到一起，基本分类为平均方法和提升方法。集成方法出名是bagging，提升方法出名的是boosting，boosting 就是‘三个臭皮匠顶个诸葛亮’的道理。

## AdaBoost算法

## AdaBoost算法的误差分析

## AdaBoost的解释

## 提升树

## Boosting 与 Bagging（Bootstrap aggregation）

## XGBoost

# EM算法

## What

EM算法是一种**迭代算法**。全称为最大期望算法（Expectation Maximization）。k-means聚类分析使用EM算法

## Why（统计学习方法）

假设有三枚硬币A、B、C，每个硬币正面出现的概率是π、p、q。进行如下的掷硬币实验：先掷硬币A，正面向上选B，反面选C；然后掷选择的硬币，正面记1，反面记0。独立的进行10次实验，结果如下：1，1，0，1，0，0，1，0，1，1。假设只能观察最终的结果(0 or 1)，而不能观测掷硬币的过程(不知道选的是B or C)，问如何估计三硬币的正面出现的概率？

## How



## EM算法在高斯混合模型中的应用

### 高斯混合分布模型



# 隐马尔可夫（HMM）

## 发展过程

- HMM - MEMM (最大熵马尔科夫模型) - CRF
## Why（wiki）

假设你有一个住得很远的朋友，他每天跟你打电话告诉你他那天做了什么。你的朋友仅仅对三种活动感兴趣：公园散步，购物以及清理房间。他选择做什么事情只凭天气。你对于他所住的地方的天气情况并不了解，但是你知道总的趋势。在他告诉你每天所做的事情基础上，你想要猜测他所在地的天气情况。

你认为天气的运行就像一个[马尔可夫链](https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE)。其有两个状态"雨"和"晴"，但是你无法直接观察它们，也就是说，它们对于你是隐藏的。每天，你的朋友有一定的概率进行下列活动："散步", "购物"，或"清理".因为你朋友告诉你他的活动，所以这些活动就是你的观察数据。这整个系统就是一个隐马尔可夫模型HMM.

你知道这个地区的总的天气趋势（今天下雨明天下雨的概率，今天下雨明天晴天的概率），并且平时知道你朋友会做的事情。

注意：只有观测序列时而没有状态序列时是采用的EM算法。

## What ?

## How?

HMM 有三个典型(canonical)问题（wiki）

###　预测：

已知模型参数和某一特定的输出序列，求最后时刻各个隐含状态的概率分布，通常使用前向算法解决

### 平滑

已知模型参数和某一特定数序序列，求中间时刻的各个隐含状态的概率分布，通常使用前向后向算法。

### 解码

已知模型参数，寻找最可能产生某一特定输出序列的隐含状态的序列，通常使用维特比算法进行解决。[维特比算法的动态图](https://en.wikipedia.org/wiki/File:Viterbi_animated_demo.gif)

另外已知输出序列，寻找最可能的状态转移及输出概率，通常使用B-W算法以及反向维特比算法。

## 概率计算方法

### 直接计算法

### 前向算法

### 后向算法

### 一些概率值和期望值的计算

## 学习算法

### 监督学习算法

### Baum—Welch算法

### B-W算法的参数估计公式

## 预测算法

### 近似算法

### 维特比算法

维特比算法是一种动态规划算法。(动态规划算法与贪婪算法的区别)

# 条件随机场（CRF）

-----

# 非监督学习

# 时间序列分析

# 问题归类

##  常见的降维方法

- LASSO，聚类，PCA，小波分析，线性判别，拉普拉斯特征映射

# 梯度下降法

## [Why](http://blog.csdn.net/coder_oyang/article/details/46544089)

y = x^2 求出y的最小值

## What

求解无约束最优化的问题

## How

### python 实现

# 牛顿法和拟牛顿法

# 拉格朗日对偶性



# 半监督学习

## 主动学习



#推荐算法
## 内容的推荐算法
## 协同过滤推荐算法
## 基于知识的推荐算法



# 常见相似度算法的原理和实现方法

## 欧式距离（Eucledian Distance）

## 曼哈顿距离（Manhattan Distance）

## 明科夫斯基距离（Minkowsi Distance）

明式距离是欧式距离的推广。

$$dist(X,Y)=(\sum_{i=1}^N | x_i - y_i |^p)^\frac{1}{p}$$

- p==1 ：曼哈顿距离
- p==2 ：欧式距离
- p==inf ：切比雪夫距离

## 余弦相似度（Cosine Similarity）

## Jaccard  Similarity